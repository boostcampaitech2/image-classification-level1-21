{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as T"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "IMG_EXTENSIONS = [\".jpg\", \".JPG\", \".jpeg\", \".JPEG\", \".png\", \".PNG\",\n",
    "                  \".ppm\", \".PPM\", \".bmp\", \".BMP\"]\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "class BaseAugmentation:\n",
    "    def __init__(self, resize, mean, std, **args):\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize(resize, Image.BILINEAR),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=mean, std=std),\n",
    "        ])\n",
    "\n",
    "    def __call__(self, image):\n",
    "        return self.transform(image)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "class AddGaussianNoise(object):\n",
    "    \"\"\"\n",
    "        transform 에 없는 기능들은 이런식으로 __init__, __call__, __repr__ 부분을\n",
    "        직접 구현하여 사용할 수 있습니다.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "class CustomAugmentation:\n",
    "    def __init__(self, resize, mean, std, **args):\n",
    "        self.transform = T.Compose([\n",
    "            T.CenterCrop((320, 256)),\n",
    "            T.Resize(resize, Image.BILINEAR),\n",
    "            T.ColorJitter(0.1, 0.1, 0.1, 0.1),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=mean, std=std),\n",
    "            AddGaussianNoise()\n",
    "        ])\n",
    "\n",
    "    def __call__(self, image):\n",
    "        return self.transform(image)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# pip install facenet-pytorch\n",
    "# ?MTCNN\n",
    "\n",
    "mtcnn  = MTCNN(post_process=False, keep_all = True)\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "\n",
    "path_to_data = \"input/data/train/images/\"\n",
    "image_path = \"000001_female_Asian_45\"\n",
    "file_names = [\"normal\", \"mask1\", \"mask2\", \"mask3\", \"mask4\", \"mask5\", \"incorrect_mask\"]\n",
    "extensions = [\".jpg\"]\n",
    "path = os.path.join(path_to_data, image_path, file_names[0] + extensions[0])\n",
    "\n",
    "img = Image.open(path)\n",
    "\n",
    "boxes, probs, points = mtcnn.detect(img, landmarks=True)\n",
    "\n",
    "print(boxes, probs, points)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[120.788864 152.77504  246.0723   329.3873  ]] [0.99389887] [[[153.74947 224.46916]\n",
      "  [214.21593 221.98465]\n",
      "  [184.88826 260.15286]\n",
      "  [160.76108 291.51437]\n",
      "  [214.89313 288.51117]]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "print(mtcnn)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MTCNN(\n",
      "  (pnet): PNet(\n",
      "    (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (prelu1): PReLU(num_parameters=10)\n",
      "    (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (conv2): Conv2d(10, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (prelu2): PReLU(num_parameters=16)\n",
      "    (conv3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (prelu3): PReLU(num_parameters=32)\n",
      "    (conv4_1): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (softmax4_1): Softmax(dim=1)\n",
      "    (conv4_2): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (rnet): RNet(\n",
      "    (conv1): Conv2d(3, 28, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (prelu1): PReLU(num_parameters=28)\n",
      "    (pool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (conv2): Conv2d(28, 48, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (prelu2): PReLU(num_parameters=48)\n",
      "    (pool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (conv3): Conv2d(48, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (prelu3): PReLU(num_parameters=64)\n",
      "    (dense4): Linear(in_features=576, out_features=128, bias=True)\n",
      "    (prelu4): PReLU(num_parameters=128)\n",
      "    (dense5_1): Linear(in_features=128, out_features=2, bias=True)\n",
      "    (softmax5_1): Softmax(dim=1)\n",
      "    (dense5_2): Linear(in_features=128, out_features=4, bias=True)\n",
      "  )\n",
      "  (onet): ONet(\n",
      "    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (prelu1): PReLU(num_parameters=32)\n",
      "    (pool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (prelu2): PReLU(num_parameters=64)\n",
      "    (pool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (prelu3): PReLU(num_parameters=64)\n",
      "    (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (conv4): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (prelu4): PReLU(num_parameters=128)\n",
      "    (dense5): Linear(in_features=1152, out_features=256, bias=True)\n",
      "    (prelu5): PReLU(num_parameters=256)\n",
      "    (dense6_1): Linear(in_features=256, out_features=2, bias=True)\n",
      "    (softmax6_1): Softmax(dim=1)\n",
      "    (dense6_2): Linear(in_features=256, out_features=4, bias=True)\n",
      "    (dense6_3): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Proposal Network (p-net)\n",
    "\n",
    "![img-pnet](https://miro.medium.com/max/1400/1*6xkYymO5qetLLjUt0MYJXg.jpeg)\n",
    "\n",
    "* This first stage is a fully convolutional network (FCN). The difference between a CNN and a FCN is that a fully convolutional network does not use a dense layer as part of the architechture. \n",
    "\n",
    "* This Proposal Network is used to obtain candidate windows and their bounding box regression vectors. [Reference](https://medium.com/@iselagradilla94/multi-task-cascaded-convolutional-networks-mtcnn-for-face-detection-and-facial-landmark-alignment-7c21e8007923)\n",
    "\n",
    "* Bounding box regression is a popular technique to predict the localization of boxes when the goal is detecting an object of some pre-defined class, in this case faces. \n",
    "\n",
    "* The final output of this stage is all candidate windows after refinement to downsize the volume of candidates.\n",
    "\n",
    "## Refine Network (r-net)\n",
    "\n",
    "![img-rnet](https://miro.medium.com/max/1400/1*PoMst7LfCfRSADzSFHXIJg.jpeg)\n",
    "\n",
    "* All candidates from the P-Net are fed into the Refine Network. Notice that this network is a CNN, not a FCN like the one before since there is a dense layer at the last stage of the network architecture. \n",
    "\n",
    "* The R-Net further reduces the number of candidates, performs calibration with bounding box regression and employs non-maximum suppression (NMS) to merge overlapping candidates.\n",
    "\n",
    "* The R-Net outputs wether the input is a face or not, a 4 element vector which is the bounding box for the face, and a 10 element vector for facial landmark localization.\n",
    "\n",
    "## Output Network (o-net)\n",
    "\n",
    "![img-onet](https://miro.medium.com/max/1400/1*GEHEFApb0VF9poTIh1Bmng.jpeg)\n",
    "\n",
    "* This stage is similar to the R-Net, but this Output Network aims to describe the face in more detail and output the five facial landmarks’ positions for eyes, nose and mouth.\n",
    "\n",
    "* There are five landmarks: left eye, right eye, nose, left mouth corner and right mouth corner.\n",
    "\n",
    "## Regarding Outputs\n",
    "\n",
    "* face classification (2개)\n",
    "\n",
    "  * y^det = GT에서 얼굴이 있는지 여부(있을때 1, 없을때 0)\n",
    "\n",
    "  * p = 얼굴이 있을 확률\n",
    "\n",
    "* bbox regression (4개)\n",
    "\n",
    "  * 예측한 bbox의 왼쪽상단 x,y좌표\n",
    "\n",
    "  * 예측한 bbox의 너비와 높이\n",
    "\n",
    "* face landmark localization (10개)\n",
    "\n",
    "  * 왼쪽 눈의 x,y 좌표\n",
    "\n",
    "  * 오른쪽 눈의 x,y 좌표\n",
    "\n",
    "  * 코의 x,y 좌표\n",
    "\n",
    "  * 입의 왼쪽 끝 부분의 x,y 좌표\n",
    "\n",
    "  * 입의 오른쪽 끝 부분의 x,y 좌표"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "for name, layer in mtcnn.named_parameters():\n",
    "    print(name, \" \\t \", layer.size())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pnet.conv1.weight  \t  torch.Size([10, 3, 3, 3])\n",
      "pnet.conv1.bias  \t  torch.Size([10])\n",
      "pnet.prelu1.weight  \t  torch.Size([10])\n",
      "pnet.conv2.weight  \t  torch.Size([16, 10, 3, 3])\n",
      "pnet.conv2.bias  \t  torch.Size([16])\n",
      "pnet.prelu2.weight  \t  torch.Size([16])\n",
      "pnet.conv3.weight  \t  torch.Size([32, 16, 3, 3])\n",
      "pnet.conv3.bias  \t  torch.Size([32])\n",
      "pnet.prelu3.weight  \t  torch.Size([32])\n",
      "pnet.conv4_1.weight  \t  torch.Size([2, 32, 1, 1])\n",
      "pnet.conv4_1.bias  \t  torch.Size([2])\n",
      "pnet.conv4_2.weight  \t  torch.Size([4, 32, 1, 1])\n",
      "pnet.conv4_2.bias  \t  torch.Size([4])\n",
      "rnet.conv1.weight  \t  torch.Size([28, 3, 3, 3])\n",
      "rnet.conv1.bias  \t  torch.Size([28])\n",
      "rnet.prelu1.weight  \t  torch.Size([28])\n",
      "rnet.conv2.weight  \t  torch.Size([48, 28, 3, 3])\n",
      "rnet.conv2.bias  \t  torch.Size([48])\n",
      "rnet.prelu2.weight  \t  torch.Size([48])\n",
      "rnet.conv3.weight  \t  torch.Size([64, 48, 2, 2])\n",
      "rnet.conv3.bias  \t  torch.Size([64])\n",
      "rnet.prelu3.weight  \t  torch.Size([64])\n",
      "rnet.dense4.weight  \t  torch.Size([128, 576])\n",
      "rnet.dense4.bias  \t  torch.Size([128])\n",
      "rnet.prelu4.weight  \t  torch.Size([128])\n",
      "rnet.dense5_1.weight  \t  torch.Size([2, 128])\n",
      "rnet.dense5_1.bias  \t  torch.Size([2])\n",
      "rnet.dense5_2.weight  \t  torch.Size([4, 128])\n",
      "rnet.dense5_2.bias  \t  torch.Size([4])\n",
      "onet.conv1.weight  \t  torch.Size([32, 3, 3, 3])\n",
      "onet.conv1.bias  \t  torch.Size([32])\n",
      "onet.prelu1.weight  \t  torch.Size([32])\n",
      "onet.conv2.weight  \t  torch.Size([64, 32, 3, 3])\n",
      "onet.conv2.bias  \t  torch.Size([64])\n",
      "onet.prelu2.weight  \t  torch.Size([64])\n",
      "onet.conv3.weight  \t  torch.Size([64, 64, 3, 3])\n",
      "onet.conv3.bias  \t  torch.Size([64])\n",
      "onet.prelu3.weight  \t  torch.Size([64])\n",
      "onet.conv4.weight  \t  torch.Size([128, 64, 2, 2])\n",
      "onet.conv4.bias  \t  torch.Size([128])\n",
      "onet.prelu4.weight  \t  torch.Size([128])\n",
      "onet.dense5.weight  \t  torch.Size([256, 1152])\n",
      "onet.dense5.bias  \t  torch.Size([256])\n",
      "onet.prelu5.weight  \t  torch.Size([256])\n",
      "onet.dense6_1.weight  \t  torch.Size([2, 256])\n",
      "onet.dense6_1.bias  \t  torch.Size([2])\n",
      "onet.dense6_2.weight  \t  torch.Size([4, 256])\n",
      "onet.dense6_2.bias  \t  torch.Size([4])\n",
      "onet.dense6_3.weight  \t  torch.Size([10, 256])\n",
      "onet.dense6_3.bias  \t  torch.Size([10])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "class PrintOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "    \n",
    "    def __call__(self, module, module_in, module_out):\n",
    "        self.outputs.append(module_out)\n",
    "        print(module_out)\n",
    "        print(module_out.size())\n",
    "    \n",
    "    def clear(self):\n",
    "        self.outputs = []\n",
    "\n",
    "print_output = PrintOutput()\n",
    "mtcnn.onet.dense6_3.register_forward_hook(print_output)\n",
    "mtcnn.onet.dense6_2.register_forward_hook(print_output)\n",
    "mtcnn.onet.dense6_1.register_forward_hook(print_output)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7fad7c9b6c10>"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "boxes, probs, points = mtcnn.detect(img, landmarks=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-2.3097,  2.3100],\n",
      "        [-1.9099,  1.9099],\n",
      "        [-2.5467,  2.5465],\n",
      "        [-1.6195,  1.6188],\n",
      "        [-2.0021,  2.0008]])\n",
      "torch.Size([5, 2])\n",
      "tensor([[ 0.1471,  0.0575, -0.1350,  0.0606],\n",
      "        [ 0.1081, -0.0103, -0.1224,  0.0411],\n",
      "        [ 0.1388, -0.0591, -0.1229, -0.0207],\n",
      "        [ 0.1100, -0.0169, -0.1379,  0.0118],\n",
      "        [ 0.1372,  0.0144, -0.1612, -0.0298]])\n",
      "torch.Size([5, 4])\n",
      "tensor([[0.3236, 0.6814, 0.5125, 0.3736, 0.6825, 0.4608, 0.4430, 0.6687, 0.8488,\n",
      "         0.8313],\n",
      "        [0.3051, 0.6690, 0.4886, 0.3549, 0.6695, 0.4170, 0.3984, 0.6276, 0.8139,\n",
      "         0.7975],\n",
      "        [0.3373, 0.6909, 0.5194, 0.3783, 0.6948, 0.3659, 0.3514, 0.5745, 0.7579,\n",
      "         0.7403],\n",
      "        [0.3024, 0.6523, 0.4771, 0.3427, 0.6616, 0.3903, 0.3789, 0.6029, 0.7830,\n",
      "         0.7709],\n",
      "        [0.3021, 0.6275, 0.4648, 0.3418, 0.6390, 0.3891, 0.3761, 0.5836, 0.7576,\n",
      "         0.7422]])\n",
      "torch.Size([5, 10])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "print(boxes, probs, points, sep = \"\\n\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[120.788864 152.77504  246.0723   329.3873  ]]\n",
      "[0.99389887]\n",
      "[[[153.74947 224.46916]\n",
      "  [214.21593 221.98465]\n",
      "  [184.88826 260.15286]\n",
      "  [160.76108 291.51437]\n",
      "  [214.89313 288.51117]]]\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}